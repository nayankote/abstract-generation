{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%time\n%%capture\n!pip install transformers\n#!pip install install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.2 -c pytorch","metadata":{"id":"rpoLx4MQdZHk","outputId":"2d040310-babb-4398-8162-bcb53709fbc9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi\n\"\"\"\nVERSION = \"20200220\" #@param [\"20200220\",\"nightly\", \"xrt==1.15.0\"]\n!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py \n%tensorflow_version 2.x\nimport tensorflow as tf\nprint(\"Tensorflow version \" + tf.__version__)\n\ntry:\n  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\nexcept ValueError:\n  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\"\"\"","metadata":{"id":"2G8FYDFfl1s8","outputId":"1f54a8a8-5775-4728-81b0-5bc2c7548dc2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport io\nimport requests\nimport numpy as np\nimport pandas as pd\nimport re\nimport zipfile\nimport random\nimport time\nimport csv\nimport datetime\nfrom itertools import compress\nfrom collections import Counter, defaultdict\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForPreTraining, \\\n                         AdamW, get_linear_schedule_with_warmup, \\\n                         TrainingArguments, BeamScorer, Trainer\n\nimport torch\nfrom torch.utils.data import Dataset, random_split, DataLoader, \\\n                             RandomSampler, SequentialSampler\n\nfrom IPython.display import clear_output\n\nprint(f\"PyTorch version: {torch.__version__}\")","metadata":{"id":"_OcqHe7jdwt5","outputId":"25cc9507-4085-4850-921f-4e2c89ee4327","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\n!wandb login \"4c7a6eeec385556acc3eb3e77d3e7e111095da18\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"import torch_xla\nimport torch_xla.core.xla_model as xm\ndev = xm.xla_device()\nt1 = torch.ones(3, 3, device = dev)\nprint(t1)\"\"\"","metadata":{"id":"Vp1RFlLpj9ii","outputId":"093fcb18-0c80-443c-cb63-e24df572d158","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndev","metadata":{"id":"_Y6ObgGzlnw1","outputId":"86f6de18-09f1-4bf8-8752-ae02213ebb84","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEBUG           = False\n\nINPUT_DIR       = 'articles'\n\nUSE_APEX        = True\nAPEX_OPT_LEVEL  = 'O1'\n\nMODEL           = \"gpt2-medium\" # \"chrisliu298/arxiv_ai_gpt2\" #'gpt2' #{gpt2, gpt2-medium, gpt2-large, gpt2-xl}\n\nUNFREEZE_LAST_N = 6 #The last N layers to unfreeze for training\n\nSPECIAL_TOKENS  = { \"bos_token\": \"<|BOS|>\",\n                    \"eos_token\": \"<|EOS|>\",\n                    \"unk_token\": \"<|UNK|>\",                    \n                    \"pad_token\": \"<|PAD|>\",\n                    \"sep_token\": \"<|SEP|>\"}\n                    \nMAXLEN          = 256  #{768, 1024, 1280, 1600}\n\nTRAIN_SIZE      = 0.9\n\nif USE_APEX:\n    TRAIN_BATCHSIZE = 8\n    BATCH_UPDATE    = 16\nelse:\n    TRAIN_BATCHSIZE = 2\n    BATCH_UPDATE    = 32\n\nEPOCHS          = 4\nLR              = 5e-4\nEPS             = 1e-8\nWARMUP_STEPS    = 1e2\n\nSEED            = 2020","metadata":{"id":"fNULlEWhd9-g","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)","metadata":{"id":"EKuR3lsveZXK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n!pip3 install kaggle\n\n# authenticate using public api token which can found from your kaggle profile page\n%env KAGGLE_USERNAME=nayannae17b010\n%env KAGGLE_KEY=f81b5ac285a14478bf294bd58c485a8f\n\n# to download on drive \n# 1) mount drive using the option of the left tab\n# 2) %cd drive/MyDrive/ \n!kaggle competitions download -c dl-hack-track-2-nlp\n!unzip \"./dl-hack-track-2-nlp.zip\"","metadata":{"id":"UsFQCkxYedFT","outputId":"c834e15a-9a97-4203-c287-d82aa4d19681","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = ['title',\n           'abstract']\ndf = pd.read_csv(\"./train.csv\", sep=\",\")\nprint(len(df))\ndf.head()","metadata":{"id":"CubnD3zjgE-b","outputId":"2394872f-c8d6-4ea1-eb55-72f50bea59c0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"word_count = 0\nfor i in range(1,len(df)) : word_count+= len(df.iloc[i][\"abstract\"].split(\" \"))\nword_count/(len(df)-1)\"\"\"\ndf['abstract_length'] = df['abstract'].apply(lambda x : len(x.split(\" \")))\nprint(df['abstract_length'].describe())\ndf.drop('abstract_length',axis=1,inplace=True)\n#sum(df['abstract_length']>256)/146381","metadata":{"id":"6S4BlBVd25O-","outputId":"4e64e9cd-59a7-4b40-9e49-7a4babbe9475","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndata = dict()\n\"\"\"   \nfor root, dirs, files in os.walk(INPUT_DIR, topdown=True):\n    t0 = time.time()\n\n    for i, f in enumerate(files):\n        #id, category, title, keywords, text\n        id = int(f[:-4])        \n        tmp = df[['CATEGORY', 'TITLE']][df.ID==id].values\n        category, title = tmp[0][0], tmp[0][1]\n\n        with open(f'{INPUT_DIR}/{f}', \"r\") as infile:\n            text = infile.read()\n        \n        data[id] = [title, text]\n\n        if i%1000==0 and i>0:\n            clear_output(wait=True)\n            print(f\"({os.getpid()}) Items processed: {i :,}/{len(files):,}; {(time.time()-t0)/60 :.1f} minutes\")\n\n            if DEBUG:\n                break\n\nprint(f\"Number of articles: {len(data) :,}\")\"\"\"\nfor i in range(len(df)):\n    data[i] = [df.iloc[i]['title'],df.iloc[i]['abstract']]","metadata":{"id":"wxxCDhwVnKeC","outputId":"2ae9208d-2037-429a-b749-88d29d080d32","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !gdown --id 1C1WWvnt2egzhRmVXMSJhARz5GOLDGzMC\n# !cp \"/content/keywords.csv.zip\" \"/content/drive/MyDrive/Colab Notebooks/dl_hack/data/\"\n# !unzip \"/content/keywords.csv.zip\" -d \"/content/drive/MyDrive/Colab Notebooks/dl_hack/data/\"\n# !head \"/content/drive/MyDrive/Colab Notebooks/dl_hack/data/keywords.csv\"","metadata":{"id":"_UVnQ2C9ib44","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class myDataset(Dataset):\n\n    def __init__(self, data, tokenizer, randomize=True):\n\n        title, text = [], [] # , keywords = [],\n        for k, v in data.items():\n            title.append(v[0])\n            text.append(v[1])\n            # keywords.append(v[2])\n\n        self.randomize = randomize\n        self.tokenizer = tokenizer \n        self.title     = title\n        self.text      = text\n        # self.keywords  = keywords  \n\n    #---------------------------------------------#\n\n    @staticmethod\n    def join_keywords(keywords, randomize=True):\n        N = len(keywords)\n\n        #random sampling and shuffle\n        if randomize: \n            M = random.choice(range(N+1))\n            keywords = keywords[:M]\n            random.shuffle(keywords)\n\n        return ','.join(keywords)\n\n    #---------------------------------------------#\n\n    def __len__(self):\n        return len(self.text)\n\n    #---------------------------------------------#\n    \n    def __getitem__(self, i):\n        # keywords = self.keywords[i].copy()\n        # kw = self.join_keywords(keywords, self.randomize)\n        \n        input = SPECIAL_TOKENS['bos_token'] + \" \" + self.title[i].strip() + \" \" + SPECIAL_TOKENS['sep_token'] + \" \" + self.text[i].strip() + \" \" + SPECIAL_TOKENS['eos_token'] # kw + SPECIAL_TOKENS['sep_token'] + \\\n\n        encodings_dict = tokenizer(input,                                   \n                                   truncation=True, \n                                   max_length=MAXLEN, \n                                   padding=\"max_length\")   \n        \n        input_ids = encodings_dict['input_ids']\n        attention_mask = encodings_dict['attention_mask']\n        \n        return {'label': torch.tensor(input_ids,device=dev),\n                'input_ids': torch.tensor(input_ids,device=dev), \n                'attention_mask': torch.tensor(attention_mask,device=dev)}","metadata":{"id":"umzOK4c3kLbD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_data(data, S=TRAIN_SIZE):\n    # Shuffle ids\n    ids = list(data.keys())\n    random.shuffle(ids)\n\n    # Split into training and validation sets    \n    train_size = int(S * len(data))\n\n    train_ids = ids[:train_size]\n    val_ids = ids[train_size:]\n\n    train_data = dict()\n    for id in train_ids:\n        train_data[id] = data[id]\n\n    val_data = dict()\n    for id in val_ids:\n        val_data[id] = data[id]\n\n    return train_data, val_data","metadata":{"id":"glSDDnkSlfHJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_tokenier(special_tokens=None):\n    tokenizer = AutoTokenizer.from_pretrained(MODEL) #GPT2Tokenizer\n\n    if special_tokens:\n        tokenizer.add_special_tokens(special_tokens)\n        print(\"Special tokens added\")\n    return tokenizer\n\ndef get_model(tokenizer, special_tokens=None, load_model_path=None):\n\n    #GPT2LMHeadModel\n    if special_tokens:\n        config = AutoConfig.from_pretrained(MODEL, \n                                            bos_token_id=tokenizer.bos_token_id,\n                                            eos_token_id=tokenizer.eos_token_id,\n                                            sep_token_id=tokenizer.sep_token_id,\n                                            pad_token_id=tokenizer.pad_token_id,\n                                            output_hidden_states=False)\n    else: \n        config = AutoConfig.from_pretrained(MODEL,                                     \n                                            pad_token_id=tokenizer.eos_token_id,\n                                            output_hidden_states=False)    \n\n    #----------------------------------------------------------------#\n    model = AutoModelForPreTraining.from_pretrained(MODEL, config=config)\n\n    if special_tokens:\n        #Special tokens added, model needs to be resized accordingly\n        model.resize_token_embeddings(len(tokenizer))\n\n    if load_model_path:\n        model.load_state_dict(torch.load(load_model_path))\n\n    model.cuda()\n    return model","metadata":{"id":"yMGAddHMlhB0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n#with tpu_strategy.scope():\ntokenizer = get_tokenier(special_tokens=SPECIAL_TOKENS)\nmodel = get_model(tokenizer, \n                special_tokens=SPECIAL_TOKENS,\n                #load_model_path='./model_v2/pytorch_model.bin'\n                )","metadata":{"id":"R6Wabd4xlzjV","outputId":"df97a591-6d31-4859-dd49-3fb02a68d72a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# - Freeze selective layers:\n# - Freeze all layers except last n:\nfor parameter in model.parameters():\n    parameter.requires_grad = False\n\nfor i, m in enumerate(model.transformer.h):        \n    #Only un-freeze the last n transformer blocks\n    if i+1 > 12 - UNFREEZE_LAST_N:\n        for parameter in m.parameters():\n            parameter.requires_grad = True \n\nfor parameter in model.transformer.ln_f.parameters():        \n    parameter.requires_grad = True\n\nfor parameter in model.lm_head.parameters():\n    parameter.requires_grad = True","metadata":{"id":"rNdHEHuUmLwa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data, val_data = split_data(data)\n\ntrain_dataset = myDataset(train_data, tokenizer)\nval_dataset = myDataset(val_data, tokenizer, randomize=False)\n\nf'There are {len(train_dataset) :,} samples for training, and {len(val_dataset) :,} samples for validation testing'","metadata":{"id":"Qv8YTXdomwAb","outputId":"39db6e28-ea91-4950-8196-f8ed1fb6c7ec","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir \"./model_med/\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ntraining_args = TrainingArguments(\n    output_dir=\"./model_med/\",\n    num_train_epochs=EPOCHS,\n    per_device_train_batch_size=TRAIN_BATCHSIZE,\n    per_device_eval_batch_size=TRAIN_BATCHSIZE,\n    gradient_accumulation_steps=BATCH_UPDATE,\n    evaluation_strategy=\"steps\",\n    fp16=True,\n    fp16_opt_level=APEX_OPT_LEVEL,\n    warmup_steps=WARMUP_STEPS,    \n    learning_rate=LR,\n    adam_epsilon=EPS,\n    weight_decay=0.01,        \n    save_total_limit=5,\n    save_strategy=\"steps\",\n    save_steps=500,\n    load_best_model_at_end=True,   \n)\n\n#---------------------------------------------------#\ntrainer = Trainer(\n    model=model,\n    args=training_args,    \n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer\n)\n\n#---------------------------------------------------#\ntrainer.train()\ntrainer.save_model()    ","metadata":{"id":"iXaGk7OTrUMb","outputId":"05a5ce57-3575-4c3d-b82b-69723ef3ab3c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\ndf_test = pd.read_csv(\"./test.csv\")\ndf_test.head()\nabstracts = []\n\"\"\"\ndevice = torch.device(\"cuda\")\nmodel.to(device)\n\"\"\"\ntokenizer.padding_side = 'left'\nmodel.eval();\nwith open(\"./predictions.csv\",'w') as csv_file:\n    \n    csv_writer = csv.writer(csv_file,delimiter=',')\n    csv_writer.writerow(['title','abstract'])\n    start = time.time()\n    for i in range(0,len(df_test),TRAIN_BATCHSIZE):\n        print(i,time.time()-start)\n        start=time.time()\n        titles = []\n        prompts = []\n        for j in range(i,min(i+TRAIN_BATCHSIZE,len(df_test))):\n            title = df_test.iloc[j]['title'].strip()\n            titles.append(title)\n            prompt = SPECIAL_TOKENS['bos_token'] + \" \" + title + \" \" + SPECIAL_TOKENS['sep_token'] + \" \"\n            prompts.append(prompt)\n\n        encoded_prompt_dict = tokenizer.batch_encode_plus(prompts, return_tensors=\"pt\", pad_to_max_length=True)\n        encoded_prompt = encoded_prompt_dict['input_ids'].to(dev)\n        encoded_mask = encoded_prompt_dict['attention_mask'].to(dev)\n        \n        ops = model.generate(encoded_prompt,\n                                       attention_mask = encoded_mask,\n                                        do_sample=True,   \n                                        min_length=100, \n                                        max_length=MAXLEN,\n                                        top_k=15,                                 \n                                        top_p=0.9,        \n                                        temperature=0.8,\n                                        repetition_penalty=2.0,\n                                        num_return_sequences=1,\n                                        length_penalty=1.0\n                                        )\n        for title,op in zip(titles,ops):\n            text = tokenizer.decode(op, skip_special_tokens=True)\n            abstract = text[len(title):]\n            csv_writer.writerow([title,abstract.strip()])\n!python3 \"../input/submission-script/submit_track2.py\" \"./predictions.csv\"\n#torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ntokenizer = get_tokenier(special_tokens=SPECIAL_TOKENS)\nmodel = get_model(tokenizer, \n                  special_tokens=SPECIAL_TOKENS,\n                  # load_model_path='../input/upto-e2/model_v1/checkpoint-2500/pytorch_model.bin'\n                 )\n                  \"\"\"","metadata":{"id":"Kk4O1mFmrYCa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\ndf_test = pd.read_csv(\"./test.csv\")\ndf_test.head()\nabstracts = []\n\"\"\"\ndevice = torch.device(\"cuda\")\nmodel.to(device)\n\"\"\"\nmodel.eval();\nwith open(\"./predictions_2.csv\",'w') as csv_file:\n    csv_writer = csv.writer(csv_file,delimiter=',')\n    csv_writer.writerow(['title','abstract'])\n    start = time.time()\n    for i in range(0,len(df_test)//100,10):\n        if i%50==0:\n            print(i,time.time()-start)\n            start=time.time()\n        title = df_test.iloc[i]['title']\n        prompt = SPECIAL_TOKENS['bos_token'] + \" \" + title.strip() + \" \" + SPECIAL_TOKENS['sep_token']\n                \n        generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n        \n        generated = generated.to(device)\n\n        \n\n        sample_output = model.generate(generated, \n                                        do_sample=True,   \n                                        min_length=50, \n                                        max_length=MAXLEN,\n                                        top_k=15,                                 \n                                        top_p=0.9,        \n                                        temperature=0.9,\n                                        repetition_penalty=2.0,\n                                        num_return_sequences=1\n                                        )\n        # print(sample_output)\n        text = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n        abstract = text[len(title):]\n        abstracts.append(abstract)\n        csv_writer.writerow([title,abstract.strip()])\n        # print(title,abstract)","metadata":{"id":"BJa_6An2bIg1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# torch.cuda.empty_cache()\ntokenizer = get_tokenier(special_tokens=SPECIAL_TOKENS)\nmodel = get_model(tokenizer, \n                  special_tokens=SPECIAL_TOKENS,\n                  # load_model_path='../input/upto-e2/model_v1/checkpoint-2500/pytorch_model.bin'\n                 )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nimport time\ndf_test = pd.read_csv(\"./test.csv\")\ndf_test.head()\nabstracts = []\n\ndevice = torch.device(\"cuda\")\nmodel.to(device)\n\ntokenizer.padding_side = 'left'\nmodel.eval();\nwith open(\"./predictions.csv\",'w') as csv_file:\n    \n    csv_writer = csv.writer(csv_file,delimiter=',')\n    csv_writer.writerow(['title','abstract'])\n    start = time.time()\n    for i in range(0,len(df_test),32):\n        print(i,time.time()-start)\n        start=time.time()\n        titles = []\n        prompts = []\n        for j in range(i,min(i+32,len(df_test))):\n            title = df_test.iloc[j]['title'].strip()\n            titles.append(title)\n            prompt = SPECIAL_TOKENS['bos_token'] + \" \" + title + \" \" + SPECIAL_TOKENS['sep_token'] + \" \"\n            prompts.append(prompt)\n\n        encoded_prompt_dict = tokenizer.batch_encode_plus(prompts, return_tensors=\"pt\", pad_to_max_length=True)\n        encoded_prompt = encoded_prompt_dict['input_ids'].to(dev)\n        encoded_mask = encoded_prompt_dict['attention_mask'].to(dev)\n        \n        ops = model.generate(encoded_prompt,\n                                       attention_mask = encoded_mask,\n                                        do_sample=True,   \n                                        min_length=100, \n                                        max_length=MAXLEN,\n                                        top_k=15,                                 \n                                        top_p=0.9,        \n                                        temperature=0.8,\n                                        repetition_penalty=2.0,\n                                        num_return_sequences=1,\n                                        length_penalty=1.0\n                                        )\n        for title,op in zip(titles,ops):\n            text = tokenizer.decode(op, skip_special_tokens=True)\n            abstract = text[len(title):]\n            csv_writer.writerow([title,abstract.strip()])\n!python3 \"../input/submission-script/submit_track2.py\" \"./predictions.csv\"\n#torch.cuda.empty_cache()\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!head \"./predictions.csv\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm \"./predictions.csv\" \"./submission.csv\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install GPUtil\n\n#import torch\n#from GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\n\ndef free_gpu_cache():\n    print(\"Initial GPU Usage\")\n    #gpu_usage()                             \n\n    torch.cuda.empty_cache()\n\n    cuda.select_device(0)\n    cuda.close()\n    cuda.select_device(0)\n\n    print(\"GPU Usage after emptying the cache\")\n    #gpu_usage()\n\nfree_gpu_cache()      ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompts = []\ntokenizer.padding_side = 'left'\nml = 0\nfor i in range(64):\n    title = df_test.iloc[i]['title'].strip(\" \")\n    ml = max(ml,len(title.split(\" \")))\n    prompt = SPECIAL_TOKENS['bos_token'] + \" \" + title.strip() + \" \" + SPECIAL_TOKENS['sep_token']\n    #tokenizer.encode(prompt,padding_side='left')\n    #generated = torch.tensor(tokenizer.encode(prompt,truncation=True, padding=\"max_length\")).unsqueeze(0)\n    #fin_generated.append(generated)\n    prompts.append(prompt)\n\ntokenizer.padding_side = \"left\"\nencoded_prompt_dict = tokenizer.batch_encode_plus(prompts, return_tensors=\"pt\", pad_to_max_length=True)\nencoded_prompt = encoded_prompt_dict['input_ids'].to(dev)\nencoded_mask = encoded_prompt_dict['attention_mask'].to(dev)\n\"\"\"\ngenerated_batch = []\nfor prompt in prompts:\n    generated_batch.append(torch.tensor(tokenizer.encode(prompt,padding='max_length',max_length=ml)))\ntorch.stack(generated_batch)\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = time.time()\nops = model.generate(encoded_prompt,\n               attention_mask =  encoded_mask,\n                                        do_sample=True,   \n                                        min_length=50, \n                                        max_length=MAXLEN,\n                                        top_k=15,                                 \n                                        top_p=0.9,        \n                                        temperature=0.9,\n                                        repetition_penalty=2.0,\n                                        num_return_sequences=1\n                                        )\nprint(time.time() - start)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python3 \"../input/submission-script/submit_track2.py\" \"./predictions.csv\"","metadata":{"id":"u0eghwkioRAy","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n\ntitle = df.iloc[1]['title']\n\nprompt = SPECIAL_TOKENS['bos_token'] + title + SPECIAL_TOKENS['sep_token']\n         \ngenerated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\ndevice = torch.device(\"cuda\")\ngenerated = generated.to(device)\nmodel.to(device)\nmodel.eval();\n\n\n# Top-p (nucleus) text generation (10 samples):\nsample_outputs = model.generate(generated, \n                                do_sample=True,   \n                                min_length=50, \n                                max_length=MAXLEN,\n                                top_k=30,                                 \n                                top_p=0.7,        \n                                temperature=0.9,\n                                repetition_penalty=2.0,\n                                num_return_sequences=10\n                                )\n\nfor i, sample_output in enumerate(sample_outputs):\n    text = tokenizer.decode(sample_output, skip_special_tokens=True)\n    a = len(title) # + len(','.join(keywords))    \n    print(\"{}: {}\\n\\n\".format(i+1,  text[a:]))\"\"\"","metadata":{"id":"rEnRDy_NwidZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"T-OLC-0xpE9X"},"execution_count":null,"outputs":[]}]}